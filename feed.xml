<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ngkel.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ngkel.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-13T20:18:10+00:00</updated><id>https://ngkel.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Optimization from basics to ISTA</title><link href="https://ngkel.github.io/blog/2025/optimization-basics/" rel="alternate" type="text/html" title="Optimization from basics to ISTA"/><published>2025-08-28T00:00:00+00:00</published><updated>2025-08-28T00:00:00+00:00</updated><id>https://ngkel.github.io/blog/2025/optimization-basics</id><content type="html" xml:base="https://ngkel.github.io/blog/2025/optimization-basics/"><![CDATA[<h1 id="introduction-to-ista">Introduction to ISTA</h1> <p>This is a blog for reviewing important optimization concepts that are critical for understanding ISTA. ISTA is an algorithm that applies proximal gradient to the LASSO objective function when solving sparse coding problem give an overcomplete dictionary. The objective function of the sparse coding with an overcomplete dictionary is as follow:</p> \[\begin{align} \min_{\mathbf{Z} \in \mathbb{R}^{d \times N}} \left\{ \left\| \mathbf{X} - \mathbf{A}\mathbf{Z} \right\|_F^2 + \lambda \|\mathbf{Z}\|_1 \right\} \end{align}\] <p>where Frobenius Norm is defined as:</p> \[\begin{align} \|\mathbf{A}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n \left| a_{ij} \right|^2} \end{align}\] <h1 id="convex-functions">Convex Functions</h1> <p>A differentiable function $\mathcal{f}: \mathcal{D} \rightarrow \mathbb{R}$ is convex if and only if</p> \[\begin{align} f(x') \geq f(x) + \nabla f(x)^*(x' - x), \forall \ x, x' \in \mathcal{D}. \end{align}\] <p>where $\mathcal{D}$ is a convex set.</p> <p>This means the graph of $f$ lies below or on the line segment joining any two points on the graph (the “chord” between them). Geometrically, convex functions have a “U-shaped” or “bowl-shaped” graph—no “dents” or local maxima that trap you from reaching the global optima.</p> <h1 id="gradient-descent">Gradient Descent</h1> <p>The well-known formula for gradient descent is:</p> \[\begin{align} x_{k+1} = x_k - h \, \nabla f(x_k) \end{align}\] <p>In gradient descent, if we find the right step size, the change of the value of the objective function has the following properties:</p> <ol> <li>In convex optimization, gradient descent improves at every iteration given correct step size.</li> <li>G.D. has self-tuning property - It takes bigger step size when it is far away from the optimal point. Vice versa. And such self-tuning property does not depend on the error at each iteration. For example, each update step of function $3x^2 +4x -2$ is $x \gets x - h (6x + 4) $ The gradient ($6x + 4$) is big when it is far away from the optimal point and smaller when it is closed. The self-tuning property is always inplace with a fixed step size, but we need to make sure the step size is chosen properly.</li> </ol> <p>These are just some useful properties for convex optimization. Things discussed below are not limited to convex optimization.</p> <p>The challenges of G.D. are the following:</p> <ol> <li>How should we determine the size $h$ such that it converges, and more importantly converges faster?</li> <li>What if the function is not differentiable at some point?</li> </ol> <h2 id="proof-of-guarantee-of-improvement-in-each-gradient-step-for-beta-smooth-function-with-step-size-being-frac1beta">Proof of guarantee of improvement in each gradient step for $\beta$-smooth function with step size being $\frac{1}{\beta}$</h2> <p>The challenge number 2 will be addressed in the next section. For the rest of this section, we will explain how step size is chosen for smooth, Lipschitz continuous function with Lipschitz constant $\beta$. Smooth function is not neccessarily convex thus we should not assume the property we are discussing here for any $\beta$-smooth function is only applicable to convex function.</p> <p>We end this section with a caveat: learning a global optimum is (usually) impractically hard. Under certain conditions, we can ensure that the gradient descent iterates converge to a local optimum. Also, under more relaxed conditions, we can ensure local convergence, i.e., that the iterates converge to a (global or local) optimum if the sequence is initialized close enough to the optimum.</p> <p>Recall that a differentiable function $f(x)$ has L-Lipschitz continuous gradients if</p> \[\begin{align} \|\nabla f(y) - \nabla f(x)\| \leq \beta \|y - x\| \quad \forall x, y \end{align}\] <p><strong>Lemma.</strong> Suppose that $f$ is differentiable and $\nabla f$ is $\beta$-Lipschitz. Then for every $x, x’ \in \mathbb{R}^n$,</p> \[\begin{align} f(x') \leq \hat{f}(x', x) = f(x) + \langle \nabla f(x), x' - x \rangle + \frac{\beta}{2} \|x' - x\|_2^2 \\ \quad\quad\;\;\; = \frac{\beta}{2} \left\| x' - \left(x - \frac{1}{\beta} \nabla f(x)\right) \right\|_2^2 + h(x). \end{align}\] <p>for some $h(x)$ that does not depend on $x’$ while $x$ is the base point. This baiscally means that we can always fit a quadratic upper bound on every point $x’$ of a Lipschitz continuous function that is $\beta$-smooth.</p> <p>We denote this upper bound as $\hat{f}(x’, x)$. The minimizer of this upper bound is</p> \[\begin{align} \arg \min_{x'} \hat{f}(x', x) = x - \frac{1}{\beta} \nabla f(x) \end{align}\] <p>This is exactly in the form of a gradient step in gradient descent. And because And because</p> \[\begin{align} f(x) = \hat{f}(x, x) \end{align}\] <p>And</p> \[\begin{align} \hat{f}(x'_{*}, x) \leq \hat{f}(x, x) \end{align}\] <p>Thus,</p> \[\begin{align} f(x'_*) \leq \hat{f}(x'_*, x) \leq \hat{f}(x, x) = f(x) \end{align}\] <p>Which imply that if we apply the gradient descent method with step size 1/$L$, we are guaranteed to produce the monotone sequence of function values $f(x_{k})$.</p> <h1 id="subgradient-method">Subgradient Method</h1> <p>To cope with the challenge number 2, we might consider subgradient method. Let’s consider the example problem:</p> \[\begin{align} f(x) = |x| \end{align}\] <p>At $x = 0$, unlike G.D., the gradient is not uniquely defined. Thus, we replace the concept gradient with another fancy term called subdifferential, which is actually a generalization of gradient. The definition of subdifferential is as follow:</p> <p>For a convex function $ f: \mathbb{R}^n \to \mathbb{R} $ the subdifferential $ \partial f(x) $ at a point $ x $ is the set of all vectors $ g \in \mathbb{R}^n $ that satisfy the subgradient inequality:</p> \[\begin{align} f(y) \geq f(x) + g^\top (y - x) \quad \forall y \in \mathbb{R}^n. \end{align}\] <p>Note that the subgradient inequality does not generally hold for non-convex problem. The key idea of subgradient method is then, for each update we do the same as G.D. except if we touch the non-differentiable point, we randomly pick a subdifferential in the set of valid subdifferentials. As the value oscillate around the non-differentiable point, we might apply some mechanism to shrink the step size. We repeat the iteration until it converges.</p> <p>The subgradient method algorithm can be summarized as follow：</p> <p>The subgradient method iteratively updates the iterate $z_k$ toward the minimum:</p> \[\begin{align} x_{k+1} = x_k - h_k g_k \end{align}\] <p>where:</p> <ul> <li> <p>$g_k$ is a subgradient at $z_k$.</p> </li> <li> <p>$h_k &gt; 0$ is the step size at iteration $k$.</p> </li> </ul> <p>Step Size Choices:</p> <ul> <li> <p>Fixed step size ($h_k = h$, $h = \frac{b}{2}$) may cause oscillation near the optimum; useful for illustration.</p> </li> <li> <p>Diminishing step size (e.g., $h_k = \frac{a}{k}$) for constants $a &gt; 0$, $b \geq 0$: Ensures convergence for bounded subgradients.</p> </li> </ul> <h2 id="optimality-condition-of-subgradient-methods">Optimality condition of subgradient methods</h2> <p>\(\text{Theorem.} \quad \text{Suppose that } f: \mathbb{R}^d \to \mathbb{R} \cup \{\infty\} \text{ is a convex function. Then, } x^* \text{ minimizes } f(x) \text{ if and only if } 0 \in \partial f(x^*)\)</p> <h2 id="convergence-rate-of-subgradient-method">Convergence rate of subgradient method</h2> <p>The main disadvantage to this approach is its relatively poor convergence rate.</p> <p>Let $x_<em>$ be the (global) minimizer of $f(x)$. In general, the convergence rate of the subgradient method for nonsmooth objective functions, in terms of the function value $f(x_k) - f(x_</em>)$, is:</p> \[O(1/\sqrt{k}).\] <p>The constants in the big-$O$ notation depend on various properties of the problem. The important point is that for even a moderate target accuracy \(\begin{align} f(x_k) - f(x_*) \leq \epsilon, \end{align}\) we will have to set $k = O(\epsilon^{-2})$ very large.</p> <h1 id="proximal-gradient-descent">Proximal Gradient Descent</h1> <p>Most objective functions such as that of ISTA problem is a composite of smooth and non-smooth functions.</p> \[\begin{align} F(x) = f(x)+g(x) \end{align}\] <p>where $f(x)$ is a $\beta$-smooth function and $g(x)$ is non-differentiable function such as $l_1$ norm. Therefore, the composite function $F(x)$ is not differentiable thus G.D. does not apply. The first recourse in this situation is to replace the gradient with a subgradient. The main disadvantage to this approach is its relatively poor convergence rate. Recall that thanks to self-tuning property of gradient descent method for smooth function, the convergence rate of G.D. is much better (at a rate of $O(1/k)$) than subgradient method. Can we draw inspiration from the gradient method to produce a more efficient algorithm for minimizing the composite function such that we can avoid slow convergence due to $x_{k+1} = x_k - h_k g_k$?</p> <p>As before, for the composite objective $F(x) = f(x) + g(x)$, we can construct an upper quadratic bound of $F(x)$ at $x_k$ using the smoothness of $f(x)$:</p> \[\hat{F}(x, x_k) = \frac{\beta}{2} \left\| x - \left( x_k - \frac{1}{\beta} \nabla f(x_k) \right) \right\|_2^2 + h(x_k) + g(x),\] \[\quad\quad\;\;\; = \hat{f}(x, x_k) + g(x).\] <p>where $\beta$ is the Lipschitz constant of the gradient of $f(x)$. Notice that $h(x_k)$ collects terms independent of $x$ (absorbing everything not affecting the minimization). This upper bound motivates the <strong>proximal gradient step</strong>:</p> <p>\(\begin{align} x_{k+1} &amp;= \arg\min_x\, \frac{\beta}{2} \left\| x - \left( x_k - \frac{1}{\beta} \nabla f(x_k) \right) \right\|_2^2 + g(x) \\ &amp;= \arg\min_x\, \frac{\beta}{2} \left\| x - w_k \right\|_2^2 + g(x) , \end{align}\) where $w_k = x_k - \frac{1}{\beta} \nabla f(x_k)$ which only depends on $x_k$.</p> <p>The minimization above is exactly the <strong>proximal operator</strong> of $g$ applied at the “gradient descent step” for $f$:</p> \[x_{k+1} = \mathrm{prox}_{g, \frac{1}{\beta}}\left( x_k - \frac{1}{\beta} \nabla f(x_k) \right) = \mathrm{prox}_{g, \frac{1}{\beta}}\left(w_k \right) = \arg\min_x\, \frac{\beta}{2} \left\| x - w_k \right\|_2^2 + g(x)\] <p>This combines the fast convergence of gradient descent for the smooth part with the regularization/control of the non-smooth part via the proximal operator. The resulting algorithm enjoys the same O(1/k) convergence rate as in the smooth case. Recognizing special structure in our problem of interest yields a significantly more accurate and eﬃcient algorithm. In fact, we can further apply acceleration method similar to acceleration method for purely smooth function to speed up the convergence! This is also the key reason how people may come up with Adam, one of the most popular optimizer in deep learning. We may talk about this in the future blog.</p> <h2 id="solving-lasso-by-proximal-gradient">Solving LASSO by proximal gradient</h2> <p>Let’s see how proximal operator looks like for LASSO problem:</p> \[\mathrm{prox}_{g, \frac{1}{\beta}}\left(w_k \right) = \arg\min_x\, \frac{\beta}{2} \left\| x - w_k \right\|_2^2 + g(x)\] <p>We can obtain the argument of the minimum by setting:</p> \[0 \in (x - w) + \lambda \partial \|x\|_1\] <p>where $w = w_k$, $\lambda = 1/\beta$, and $\partial |x|_1$ denotes the subdifferential of the $\ell_1$ norm. For each coordinate $i = 1, \ldots, n$:</p> <p>\(0 \in (x_i - w_i) + \lambda \partial |x_i| = \begin{cases} x_i - w_i + \lambda, &amp; x_i &gt; 0 \\ -w_i + \lambda[-1, 1], &amp; x_i = 0 \\ x_i - w_i - \lambda, &amp; x_i &lt; 0 \end{cases}\) Therefore, the solution to this optimality condition is the <strong>soft-thresholding function applied element-wise</strong>: \(x_i^* = \mathrm{soft}(w_i, \lambda) \triangleq \mathrm{sign}(w_i) \max(|w_i| - \lambda, 0) \qquad i = 1, \ldots, n.\)</p> <p>The pseudocode of PGD for LASSO problem is as follow:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pgd_for_lasso-480.webp 480w,/assets/img/pgd_for_lasso-800.webp 800w,/assets/img/pgd_for_lasso-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/pgd_for_lasso.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>As the iteration goes on, more and more value in the solution vector may go zero to promote sparsity. The soft-threshold is determined by $\frac{\lambda}{\beta}$ (replace $L$ by $\beta$ in the picture)</p> <h2 id="similarity-of-proximal-operator-and-nonlinear-function-in-deep-network">Similarity of proximal operator and nonlinear function in deep network</h2> <p>The soft-thresholding in LISTA looks notably similar to ReLU in deep network.</p> <h1 id="how-solution-to-lasso-inspire-more-efficient-solution-to-basis-pursuit">How solution to LASSO inspire more efficient solution to basis pursuit</h1> <p>When I first learn about these 2 problems, most textbook would introduce them as following roughly: LASSO is a variant of the BP, considering the noisy case in which the original signal is contaminated by moderate Gaussian noise $\mathcal{z}$.</p> <p>Basis Pursuit:</p> \[\begin{aligned} &amp;\min_{x} \quad &amp;&amp; \|x\|_1 \\ &amp;\text{subject to} \quad &amp;&amp; Ax = y. \end{aligned}\] <p>LASSO:</p> \[\begin{align} \min_{x} \ \frac{1}{2} \|y - Ax\|_2^2 + \lambda \|x\|_1 \end{align}\] <p>where the observations $y$ are generated as $y = Ax_0 + z$, for some unknown sparse $x_0$ and noise $z$.</p> <p>However, after I went through the optimization textbook or chapters in related textbook over and over again, I have found that there is a deeper and valuable insight. First of all, let’s recall the core difference between 2 problems. BP or BPDN enforce equality or inequality constraints while LASSO does not strictly do so. In LASSO, it is up to us to control the $\lambda$.</p> <p>Because of the requirement to enforce equality constraint on BP, the first intuitive method comes to people mind is to apply projected subgradient method to solve BP. The key idea of BP is to alternate between subgradient steps and projection operation. The convergence rate thus is mainly determined by subgradient step for non-smooth and non-strongly convex $|x|_1$, given by:</p> \[O(1/\sqrt{k}).\] <p>The constants in the big-$O$ notation depend on various properties of the problem. The important point is that for even a moderate target accuracy \(\begin{align} f(x_k) - f(x_*) \leq \epsilon, \end{align}\) we will have to set $k = O(\epsilon^{-2})$ very large.</p> <p>Apart from using projected subgradient method to solve this problem, another approach is to solve a series of problems of this type :</p> \[\begin{align} \min_x \ \|x\|_1 + \frac{\mu}{2} \|y - Ax\|_2^2 \end{align}\] <p>for an increasing sequence of $\mu_i \to \infty$ to enforce the equality.</p> <p>Now, the deeper insight I have mentioned will be explained starting from here.</p> <p>First of all, it is obvious that for each $\mu_i$, the problem is exactly a LASSO problem, which can be solved by proximal gradient descent (PGD).</p> <p>PGD, unlike projected subgradient descent, benefit from the smooth part $|y - Ax|_2^2$ and thus the convergence rate is $O(1/k)$. It also allows us to utilize acceleration techniques that were designed for smooth functions and lead to much more scalable and fast-converging algorithms, with convergence rates much better than the generic situation. Therefore, in each iteration for each $\mu_i$. we solve the problem more efficiently. However, as the weight $\mu_i$ increases, the corresponding BPDN problem becomes increasingly ill-conditioned and hence algorithms converge slower. This is because for first-order methods such as PG and APG, the rate of convergence is dictated by how quickly the gradient $\nabla(\mu f) = \mu A^T (A x - y)$ can change from point-to-point, which is measured through the Lipschitz constant.</p> \[\begin{align} L_{\nabla_{\mu}f} = \mu \|\mathbf{A}\|_2^2. \end{align}\] <p>This increases linearly with $\mu$: The larger $\mu$ is, the harder the unconstrained problem is to solve! We can then employ the augmented Lagrange multiplier (ALM) technique to alleviate this diﬃculty.</p> <h1 id="connection-between-ista-and-deep-network">Connection between ISTA and deep network</h1> <p>Recall that in each layer of forward pass of feature transformation in ISTA is given by the following process:</p> \[\begin{align} \mathbf{Z}_{\ell+1} = S_{\eta, \lambda} \left( \mathbf{Z}_\ell - 2\eta (\mathbf{A}_\ell)^\top \left( \mathbf{A}_\ell \mathbf{Z}_\ell - \mathbf{X} \right) \right),\quad \forall \ell \in [L]. \end{align}\] <p>It looks like the forward pass of a deep neural network with weights given by $\mathbf{A}$. The first part of this blog</p>]]></content><author><name>Ng Ka Lok</name></author><category term="optimization"/><summary type="html"><![CDATA[Introduction to ISTA]]></summary></entry><entry><title type="html">What to learn and introduction to problems solvable by analytical approach</title><link href="https://ngkel.github.io/blog/2025/what-to-learn/" rel="alternate" type="text/html" title="What to learn and introduction to problems solvable by analytical approach"/><published>2025-08-18T00:00:00+00:00</published><updated>2025-08-18T00:00:00+00:00</updated><id>https://ngkel.github.io/blog/2025/what-to-learn</id><content type="html" xml:base="https://ngkel.github.io/blog/2025/what-to-learn/"><![CDATA[<h1 id="what-learning-is-all-about-or-at-least-what-are-modern-methods-actually-doing">What learning is all about? Or at least, what are modern methods actually doing?</h1> <p>In nature, data in high dimensional space has low dimensional support. Data lie precisely on geomoetric structures such as subspaces or surfaces. Such constraint make data highly dependent on one another. In other words, natural data is predictable. Completion is perhaps another name of prediction. Denoise and error correction, are tasks that are enabled by the fact that data has constraints.</p> <p>Thus, we can say that learning is useful as it aims at, first, identifying and representing the low-dimensional structure of data. Next, we utilize the low-dimensional structure to inference for useful downstream tasks.</p> <p>Our questions then become:</p> <ol> <li>How to learn and represent the data</li> <li>How to make use of the representation obtained from 1 to conduct useful inference.</li> </ol> <p><strong><em>In this series of blogs, we tackle these questions by claiming that we learn and represent the data as distribution $p(x)$ with low dimenstional structure. Then we make use of $p(x)$ for inferencing.</em></strong></p> <h1 id="learning-problems-that-are-solvable-by-analytical-approach">Learning problems that are solvable by analytical approach</h1> <p>Analytical approaches involve explicit low-dimensional structure being assumed while empirical do not. In this section, we discuss classical problem settings and solutions for learning models with assumption that data having geometrically (nearly, piece-wise) linear structures and statistically independent components. Such model is what we call analytical models. Such assumptions offer us efficient algorithm with provable efficiency guarantees for processing data at scale. You may ask why we are learning these models when the real world data is far more complex? We will see in the upcoming blogs that many modern deep learning architectures for complex data actually have structures that are similar to algorithm for modeling data with linear and independent structures, such as overcomplete dictionary learning.</p> <p>Learning distribution with idealistic, linear models with independent structures has efficient solutions. Examples include:</p> <ol> <li>PCA</li> <li>ICA</li> <li>Sparse Coding and Overcomplete Dictionary Learning</li> </ol> <p>Distributions for the above modeling problems can be learned and represented explicitly. We will see later that general distribution does not yield efficient analytical solution and cannot be represented explicitly but they can be learnt implicitly as a denoiser.</p> <p><strong><em>Reminder on what we mean by explicit family of parametric model: The model class that is directly and mathematically specified, for example, “all Gaussians with arbitary mean and covariance”. When learning this type of model, we fix the form of the model and tune the set of parameters. Neural networks on the other hands, are considered empirical approaches without clearly stating analytically the form of distribution they are trying to learn. One might argue that the network architecture may limit or define the type of distribution they are able to learn, but the design process of neural networks does not involve explicitly define the form of distribution with clear mathematical descriptions.</em></strong></p> <h2 id="pca---finding-the-single-subspace-that-best-fits-the-data">PCA - finding the single subspace that best fits the data</h2> <p>In PCA, data is assumed, explicitly, to live on a single Gaussian subspace $\mathcal{S} \subseteq \mathbb{R}^D$ of dimension $d$ with basis represented by an orthonormal matrix $\mathbf{U} \in O(D, d) \subseteq \mathbb{R}^{D \times d}$ such that the columns of $\mathbf{U}$ span $\mathcal{S}$. The problem of learning thus becomes:</p> <p>Given observed data ${x_i}_{i=1}^N$, finding the orthonoraml matrix $\mathbf{U} \in O(D,d)$ such that</p> \[\begin{align} \mathbf{x}_i = \mathbf{U} \mathbf{z}_i + \boldsymbol{\varepsilon}_i, \quad \forall i \in [N] \end{align}\] <p>where</p> \[\begin{align} \{\mathbf{z}_i\}_{i=1}^N \subseteq \mathbb{R}^d, \quad d \ll D \end{align}\] <p>The optimization problem for finding the optimal subspace $\mathbf{U}^*$ can be formulated as:</p> \[\begin{align} \arg \min_{\tilde{\mathbf{U}}} \frac{1}{N} \sum_{i=1}^{N} \| \mathbf{x}_i - \tilde{\mathbf{U}} \tilde{\mathbf{U}}^T \mathbf{x}_i \|_2^2 &amp;= \arg \max_{\tilde{\mathbf{U}}} \frac{1}{N} \sum_{i=1}^{N} \| \tilde{\mathbf{U}}^T \mathbf{x}_i \|_F^2 \\ &amp;= \arg \max_{\tilde{\mathbf{U}}} \text{tr} \left\{ \tilde{\mathbf{U}}^T \left( \frac{\mathbf{X} \mathbf{X}^T}{N} \right) \tilde{\mathbf{U}} \right\} \end{align}\] <p>That is, the optimal solution $(\mathbf{U}^<em>, {\mathbf{z}^</em><em>i}</em>{i=1}^N)$ to the above optimization problem has $\mathbf{z}^<em>_i = (\mathbf{U}^</em>)^T \mathbf{x}_i$. That’s why the optimization problem above becomes a problem over $\mathbf{U}$ only. The solution of the above optimization problem is given by the top $d$ eigenvectors of:</p> \[\begin{align} \frac{\mathbf{X} \mathbf{X}^T}{N} \end{align}\] <p>It is worth noting that projection matrix:</p> \[\begin{align} \mathbf{U}^* (\mathbf{U}^*)^T \approx \mathbf{U} \mathbf{U}^T \end{align}\] <p>can project the noisy data point $\mathbf{x}_i$ onto subspace $\mathcal{S}$.</p> <p>Reminder of linear algebra concept: Matrix multiplication, let’s say multiplied by $\mathbf{U}$ has 3 interpretations:</p> <ol> <li>Transformation of a vector</li> <li>Express original vector $\mathbf{x}$ in terms of a vector based on coordinate provided by $\mathbf{U}$.</li> </ol> <h2 id="power-iteration---workhorse-of-pca">Power Iteration - Workhorse of PCA</h2> <p>Computing full SVD for a matrix is computationally intensive. There is, however, an efficient way to comput the eigenvector of a symmetric, positive semidefinite matrix $\mathbf{M}$. The method is called power iteration.</p> <p>Power iteration is a fundamental algorithm for finding the dominant eigenvector of a matrix $\mathbf{M}$. The key insight is that repeatedly applying the matrix and normalizing will converge to the eigenvector corresponding to the largest eigenvalue.</p> <p><strong>The Algorithm.</strong></p> <p>Assume that $\lambda_1 &gt; \lambda_i$ for all $i &gt; 1$. We want to find the fixed point of:</p> \[\begin{align} \mathbf{w} = \frac{\mathbf{M}\mathbf{w}}{\|\mathbf{M}\mathbf{w}\|_2} \end{align}\] <p>We can solve this using the following iterative procedure:</p> \[\begin{align} \mathbf{v}_0 \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), \quad \mathbf{v}_{t+1} = \frac{\mathbf{M}\mathbf{v}_t}{\|\mathbf{M}\mathbf{v}_t\|_2} \end{align}\] <p><strong>Why does this work?</strong></p> <p><strong>Proof Sketch.</strong></p> <p>For any iteration $t$, we have:</p> \[\begin{align} \mathbf{v}_t = \frac{\mathbf{M}^t \mathbf{v}_0}{\|\mathbf{M}^t \mathbf{v}_0\|_2} \end{align}\] <p>Let’s decompose the initial vector $\mathbf{v}_0$ in the eigenbasis of $\mathbf{M}$:</p> \[\begin{align} \mathbf{v}_0 = \sum_{i=1}^D \alpha_i \mathbf{w}_i \end{align}\] <p>where $\mathbf{M}\mathbf{w}_i = \lambda_i \mathbf{w}_i$ and $\lambda_1 &gt; \lambda_2 \geq \dotsb \geq \lambda_D \geq 0$.</p> <p>Substituting this into our iteration formula:</p> \[\begin{align} \mathbf{v}_t = \frac{ \sum_{i=1}^D \lambda_i^t \alpha_i \mathbf{w}_i }{ \left\| \sum_{i=1}^D \lambda_i^t \alpha_i \mathbf{w}_i \right\|_2 } \end{align}\] <p>As $t \to \infty$, since $\lambda_1 &gt; \lambda_i$ for $i &gt; 1$, the terms with $i &gt; 1$ vanish exponentially faster than the first term. This gives us:</p> \[\begin{align} \lim_{t \to \infty} \mathbf{v}_t = \frac{ \alpha_1 \mathbf{w}_1 }{ |\alpha_1| } = \operatorname{sign}(\alpha_1)\mathbf{w}_1 \end{align}\] <p>Therefore, $\mathbf{v}_t$ converges to a unit eigenvector of $\mathbf{M}$ corresponding to the largest eigenvalue.</p> <p><strong>Estimating the eigenvalue.</strong></p> <p>Once we have the eigenvector, the corresponding eigenvalue can be estimated as:</p> \[\begin{align} \lambda_1 \approx \mathbf{v}_t^\top \mathbf{M} \mathbf{v}_t \end{align}\] <p>This quantity converges to $\lambda_1$ at the same rate as the eigenvector convergence.</p> <p>Implementation:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
</pre></td><td class="code"><pre><span class="k">import</span> <span class="n">numpy</span> <span class="n">as</span> <span class="n">np</span>

<span class="n">def</span> <span class="n">power_iteration</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span><span class="o">:</span>
    <span class="s">"""</span><span class="err">
</span><span class="s">    Compute the dominant eigenvalue and eigenvector of a symmetric positive semidefinite matrix</span><span class="err">
</span><span class="s">    using the Power Iteration method.</span><span class="err">

</span><span class="s">    Parameters:</span><span class="err">
</span><span class="s">    M (numpy.ndarray): Symmetric positive semidefinite matrix</span><span class="err">
</span><span class="s">    num_iterations (int): Maximum number of iterations</span><span class="err">
</span><span class="s">    tol (float): Convergence tolerance</span><span class="err">

</span><span class="s">    Returns:</span><span class="err">
</span><span class="s">    tuple: (eigenvalue, eigenvector)</span><span class="err">
</span><span class="s">    """</span>
    <span class="cp"># Input validation
</span>    <span class="k">if</span> <span class="n">not</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">M</span><span class="p">.</span><span class="n">T</span><span class="p">)</span><span class="o">:</span>
        <span class="n">raise</span> <span class="n">ValueError</span><span class="p">(</span><span class="s">"Matrix must be symmetric"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">not</span> <span class="n">np</span><span class="p">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">M</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="n">tol</span><span class="p">)</span><span class="o">:</span>  <span class="err">#</span> <span class="n">Allow</span> <span class="n">small</span> <span class="n">negative</span> <span class="n">values</span> <span class="n">due</span> <span class="n">to</span> <span class="n">numerical</span> <span class="n">errors</span>
        <span class="n">raise</span> <span class="n">ValueError</span><span class="p">(</span><span class="s">"Matrix must be positive semidefinite"</span><span class="p">)</span>

    <span class="cp"># Initialize random vector
</span>    <span class="n">n</span> <span class="o">=</span> <span class="n">M</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>  <span class="err">#</span> <span class="n">Normalize</span> <span class="n">initial</span> <span class="n">vector</span>

    <span class="cp"># Power iteration
</span>    <span class="k">for</span> <span class="n">_</span> <span class="n">in</span> <span class="n">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">)</span><span class="o">:</span>
        <span class="cp"># Matrix-vector multiplication
</span>        <span class="n">v_new</span> <span class="o">=</span> <span class="n">M</span> <span class="err">@</span> <span class="n">v</span>

        <span class="cp"># Compute eigenvalue (Rayleigh quotient)
</span>        <span class="n">eigenvalue</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">v_new</span><span class="p">)</span>

        <span class="cp"># Normalize the new vector
</span>        <span class="n">v_new_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v_new</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">v_new_norm</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="o">:</span>  <span class="err">#</span> <span class="n">Check</span> <span class="k">for</span> <span class="n">zero</span> <span class="n">vector</span>
            <span class="n">raise</span> <span class="n">ValueError</span><span class="p">(</span><span class="s">"Power iteration converged to zero vector"</span><span class="p">)</span>
        <span class="n">v_new</span> <span class="o">=</span> <span class="n">v_new</span> <span class="o">/</span> <span class="n">v_new_norm</span>

        <span class="cp"># Check convergence
</span>        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">v_new</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tol</span> <span class="n">or</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v</span> <span class="o">+</span> <span class="n">v_new</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="o">:</span>
            <span class="k">break</span>

        <span class="n">v</span> <span class="o">=</span> <span class="n">v_new</span>

    <span class="k">return</span> <span class="n">eigenvalue</span><span class="p">,</span> <span class="n">v</span>

<span class="cp"># Example usage
</span>
<span class="cp"># Create a sample 3x3 symmetric positive semidefinite matrix
</span><span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>

<span class="cp"># Run power iteration
</span><span class="n">eigenvalue</span><span class="p">,</span> <span class="n">eigenvector</span> <span class="o">=</span> <span class="n">power_iteration</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>

<span class="n">print</span><span class="p">(</span><span class="s">"Dominant eigenvalue:"</span><span class="p">,</span> <span class="n">eigenvalue</span><span class="p">)</span>
<span class="n">print</span><span class="p">(</span><span class="s">"Corresponding eigenvector:"</span><span class="p">,</span> <span class="n">eigenvector</span><span class="p">)</span>

<span class="cp"># Verify result
</span><span class="n">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Verification:"</span><span class="p">)</span>
<span class="n">print</span><span class="p">(</span><span class="s">"M @ eigenvector:"</span><span class="p">,</span> <span class="n">M</span> <span class="err">@</span> <span class="n">eigenvector</span><span class="p">)</span>
<span class="n">print</span><span class="p">(</span><span class="s">"eigenvalue * eigenvector:"</span><span class="p">,</span> <span class="n">eigenvalue</span> <span class="o">*</span> <span class="n">eigenvector</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <h2 id="limitation-of-pca">Limitation of PCA</h2> <p>PCA can denoise data by projecting original data on a single principal subspace. However, it has 2 key limitations:</p> <ol> <li>It fails to model nonlinear structure</li> <li>Real world data comes from multiple subspaces or surfaces. PCA also fails to model.</li> </ol> <p>The following 3D interactive plots demonstrate visually and computationally why PCA is limited when the data lies on a nonlinear manifold (e.g., a sine wave in 3D). The PCA algorithm is applied to the noisy data, attempting to find the best linear subspace that fits the data. PCA projects the data onto the top 2 principal components, which are linear. The denoised result is obtained by reconstructing the data from this 2D linear subspace. However, the learned model does not offer generative model for nonlinear distribution, since only variance along the principal direction is captured. That is not enough to describe a sinusoidal relationship.</p> <div class="l-page" style="display: flex; justify-content: center; gap: 20px; flex-wrap: wrap;"> <div style="flex: 1; min-width: 400px;"> <iframe src="/assets/plotly/blog_2025_08_18/pca_sin_wave_noisy.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div style="flex: 1; min-width: 400px;"> <iframe src="/assets/plotly/blog_2025_08_18/pca_sin_wave_denoised.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> </div> <div> <iframe src="/assets/plotly/blog_2025_08_18/pca_sin_wave_generative_model.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <h2 id="mixtures-of-subspaces-and-sparsely-used-dictionaries">Mixtures of Subspaces and Sparsely-Used Dictionaries</h2> <p>A key limitation is the assumption of a single linear subspace that is responsible for generating the structured observations. In many practical applications, structure generated by a mixture of distinct low-dimensional subspaces provides a more realistic model. For example, consider a video sequence that captures the motion of several distinct objects, each subject to its own independent displacement.</p> <p>In mixture of Gaussians, the random variable is generated by, randomly select a Gaussian from K Gaussians, then randomly select a random variable in that selected Gaussian.</p> <p>The probability density function can be written as: \(\begin{align} p(\mathbf{x}_n) = \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \end{align}\)</p> <p>Please don’t confuse this with superposition: \(\begin{align} \mathbf{x} = \sum_{i=1}^{n} w_i \mathbf{x}_i, \quad \mathbf{x}_i \sim \mathcal{N}(\mathbf{0}, \mathbf{U}_i \mathbf{U}_i^\top) \end{align}\)</p> <p>After assuming a mixture of Gaussian model, our task is to learn $\mathbf{U}$ which satisfy the following:</p> \[\begin{align} \mathbf{x} = \begin{bmatrix} | &amp; &amp; | \\ \mathbf{U}_1 &amp; \cdots &amp; \mathbf{U}_K \\ | &amp; &amp; | \end{bmatrix} \begin{bmatrix} \mathbf{z}_1 \\ \vdots \\ \mathbf{z}_K \end{bmatrix} = \mathbf{U}\, \mathbf{z}, \quad \left\| \begin{bmatrix} \|\mathbf{z}_1\|_2 \\ \vdots \\ \|\mathbf{z}_K\|_2 \end{bmatrix} \right\|_0 = 1 \end{align}\] <ul> <li>\(\mathbf{U} \in \mathbb{R}^{D \times (K d)}\) is a <em>dictionary</em> consisting of the collection of codewords \(\{\mathbf{U}_i\}_{i=1}^K\).</li> <li>\(\mathbf{z} \in \mathbb{R}^{K d}\) is a <em>lifted vector</em> that is \(d\)-sparse, with support on one block of size \(d\).</li> </ul> <p>where $\mathbf{U} \in \mathbb{R}^{D \times Kd}$</p> <p>A relaxation is to assume matrix $\mathbf{U} \in \mathbb{R}^{D \times m}$ where $m$ may be smaller or larger than $D$. This leads to sparse dictionary learning problem. There are both geometric and physical/modeling motivations for considering $d \ll m$. The problem is thus turned to be an overcomplete dictionary learning problem. The motivation includes having a richer mixtures of subspaces and some computational experiments in the past reveals the additional modeling power conferred by an overcomplete representation, for example, Bruno Olshausen’s paper on overcomplete dictionary learning.</p> <h2 id="overcomplete-dictionary-learning">Overcomplete dictionary learning</h2> <p>To learn $\mathbf{U}$, the corresponding optimization problem can be written as follow:</p> \[\begin{align} \min_{\tilde{\mathbf{Z}},\, \tilde{\mathbf{A}}: \|\tilde{\mathbf{A}}_j\|_2 \leq 1} \Big\{\, \|\mathbf{X} - \tilde{\mathbf{A}}\, \tilde{\mathbf{Z}}\,\|_F^2 + \lambda \|\tilde{\mathbf{Z}}\|_1 \, \Big\} \end{align}\] <p>There are multiple things worth a separate blog to discuss in this optimization problem:</p> <ol> <li>The constraint of having norm in each basis vector of matrix $\mathbf{A}$</li> <li>Such minimization problem is non-convex. How to minimize it with alternating minization of $\mathbf{Z}$ and $\mathbf{A}$?</li> <li>How does the number of rows affect the “quality” of the features learnt?</li> </ol> <p>Despite the dictionary learning problem being a nonconvex problem, it is easy to see that fixing one of the 2 unknowns and optimize the other makes the problem convex and easy to solve. It has been shown that alternating minimization type algorithms indeed converge to the correct solution, at least locally. The algorithm is as follow:</p> \[\begin{align} \mathbf{Z}^{\ell+1} = S_{\eta\lambda}\left(\mathbf{Z}^{\ell} - 2 \eta\, \mathbf{A}^{+\top} \left(\mathbf{A}^+ \mathbf{Z}^{\ell} - \mathbf{X}\right)\right),\quad \mathbf{Z}^1=\mathbf{0},\quad \forall\, \ell \in [L] \end{align}\] \[\begin{align} \mathbf{Z}^+ = \mathbf{Z}^L \end{align}\] \[\begin{align} \mathbf{A}^{t+1} = \mathrm{proj}_{\|(\cdot)_j\|_2 \leq 1,\, \forall j}\left(\mathbf{A}^t - 2\nu\, (\mathbf{A}^t \mathbf{Z}^+ - \mathbf{X}) (\mathbf{Z}^+)^\top \right) \end{align}\] \[\begin{align} (\mathbf{A}^1)_j \sim \mathrm{i.i.d.}\, \mathcal{N}\left( \mathbf{0}, \frac{1}{D}\mathbf{I} \right),\quad \forall j \in [m] ,\quad \forall t \in [T] \end{align}\] \[\begin{align} \mathbf{A}^+ = \mathbf{A}^T \end{align}\] <p>The main insight from the above algorithm is that, when we fix $\mathbf{A}$, the ISTA update of $\mathbf{Z}^{\ell}$ looks like the forward pass of neural networks. Then the using the thrid line to update $\mathbf{A}$ based on the residual of using the current estimate of the sparse codes $\mathbf{Z}$</p> <p>(Draft) Equation 18 and 19 are ISTA algorithm for LASSO problem.</p> <h2 id="ista---an-algorithm-for-solving-sparse-coding-in-overcomplete-dictionary-learning">ISTA - An algorithm for solving sparse coding in overcomplete dictionary learning</h2> <h1 id="learned-ista-lista">Learned ISTA (LISTA)</h1> <p>The above deep-network interpretation of the alternating miniziation is more conceptual than practical, as the process can be rather inefficient and take many layers or iterations to converge. This is because we are trying to infer both $\mathbf{A}$ and $\mathbf{Z}$ from $\mathbf{X}$. The problem can be simplified in a supervised setting where $(\mathbf{X}, \mathbf{Z})$ are provided and use, say, back propagation type of algorithm to learn $\mathbf{A}$.</p> <p>The same methodology can be used as a basis to understand the representations computed in other network architectures, such as Transformer. Modern unsupervised learning paradigms are generally more data friendly but still, LISTA algorithm provide a useful practical basis for us to interpret the features in pretrained large-scale deep networks.</p> <p>The connection between low-dimensional-structure-seeking optimization algorithms and deep network architecture suggests scalable and natural neural learning architectures which may even be usable without backpropagation.</p> <h2 id="sparse-autoencoders">Sparse Autoencoders</h2> <p>Unlike LISTA, which requires large amounts of labeled $(\mathbf{X}, \mathbf{Z})$, unsupervised learning paradigm is more data friendly. We can use our development of the LISTA algorithm above to understand common practices in this field of research.</p> <p>Connection of LISTA to Sparse Autoencoder</p>]]></content><author><name>Ng Ka Lok</name></author><category term="representation_learning"/><summary type="html"><![CDATA[Idealistic models that inspire deep network structures]]></summary></entry></feed>