<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Optimization from basics to ISTA | Ka Lok Ng </title> <meta name="author" content="Ka Lok Ng"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.css" integrity="sha256-VwMV//xgBPDyRFVSOshhRhzJRDyBmIACniLPpeXNUdc=" crossorigin="anonymous"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ngkel.github.io/blog/2025/optimization-basics/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.css" integrity="sha256-q9ba7o845pMPFU+zcAll8rv+gC+fSovKsOoNQ6cynuQ=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css" integrity="sha256-Oppd74ucMR5a5Dq96FxjEzGF7tTw2fZ/6ksAqDCM8GY=" crossorigin="anonymous" media="screen and (prefers-color-scheme: light)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" integrity="sha256-nyCNAiECsdDHrr/s2OQsp5l9XeY2ZJ0rMepjCT2AkBk=" crossorigin="anonymous" media="screen and (prefers-color-scheme: dark)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/css/diff2html.min.css" integrity="sha256-IMBK4VNZp0ivwefSn51bswdsrhk0HoMTLc2GqFHFBXg=" crossorigin="anonymous"> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Optimization from basics to ISTA",
            "description": "",
            "published": "August 28, 2025",
            "authors": [
              
              {
                "author": "Ng Ka Lok",
                "authorURL": "ngkel.github.io",
                "affiliations": [
                  {
                    "name": "Ex-HKU, Foodpanda",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ka Lok</span> Ng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">miscellaneous </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Optimization from basics to ISTA</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction-to-ista">Introduction to ISTA</a> </div> <div> <a href="#convex-functions">Convex Functions</a> </div> <div> <a href="#gradient-descent">Gradient Descent</a> </div> <div> <a href="#subgradient-method">Subgradient Method</a> </div> <div> <a href="#proximal-gradient-descent">Proximal Gradient Descent</a> </div> <div> <a href="#how-solution-to-lasso-inspire-more-efficient-solution-to-basis-pursuit">How solution to LASSO inspire more efficient solution to basis pursuit</a> </div> <div> <a href="#connection-between-ista-and-deep-network">Connection between ISTA and deep network</a> </div> </nav> </d-contents> <h1 id="introduction-to-ista">Introduction to ISTA</h1> <p>This is a blog for reviewing important optimization concepts that are critical for understanding ISTA. ISTA is an algorithm that applies proximal gradient to the LASSO objective function when solving sparse coding problem give an overcomplete dictionary. The objective function of the sparse coding with an overcomplete dictionary is as follow:</p> \[\begin{align} \min_{\mathbf{Z} \in \mathbb{R}^{d \times N}} \left\{ \left\| \mathbf{X} - \mathbf{A}\mathbf{Z} \right\|_F^2 + \lambda \|\mathbf{Z}\|_1 \right\} \end{align}\] <p>where Frobenius Norm is defined as:</p> \[\begin{align} \|\mathbf{A}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n \left| a_{ij} \right|^2} \end{align}\] <h1 id="convex-functions">Convex Functions</h1> <p>A differentiable function $\mathcal{f}: \mathcal{D} \rightarrow \mathbb{R}$ is convex if and only if</p> \[\begin{align} f(x') \geq f(x) + \nabla f(x)^*(x' - x), \forall \ x, x' \in \mathcal{D}. \end{align}\] <p>where $\mathcal{D}$ is a convex set.</p> <p>This means the graph of $f$ lies below or on the line segment joining any two points on the graph (the “chord” between them). Geometrically, convex functions have a “U-shaped” or “bowl-shaped” graph—no “dents” or local maxima that trap you from reaching the global optima.</p> <h1 id="gradient-descent">Gradient Descent</h1> <p>The well-known formula for gradient descent is:</p> \[\begin{align} x_{k+1} = x_k - h \, \nabla f(x_k) \end{align}\] <p>In gradient descent, if we find the right step size, the change of the value of the objective function has the following properties:</p> <ol> <li>In convex optimization, gradient descent improves at every iteration given correct step size.</li> <li>G.D. has self-tuning property - It takes bigger step size when it is far away from the optimal point. Vice versa. And such self-tuning property does not depend on the error at each iteration. For example, each update step of function $3x^2 +4x -2$ is $x \gets x - h (6x + 4) $ The gradient ($6x + 4$) is big when it is far away from the optimal point and smaller when it is closed. The self-tuning property is always inplace with a fixed step size, but we need to make sure the step size is chosen properly.</li> </ol> <p>These are just some useful properties for convex optimization. Things discussed below are not limited to convex optimization.</p> <p>The challenges of G.D. are the following:</p> <ol> <li>How should we determine the size $h$ such that it converges, and more importantly converges faster?</li> <li>What if the function is not differentiable at some point?</li> </ol> <h2 id="proof-of-guarantee-of-improvement-in-each-gradient-step-for-beta-smooth-function-with-step-size-being-frac1beta">Proof of guarantee of improvement in each gradient step for $\beta$-smooth function with step size being $\frac{1}{\beta}$</h2> <p>The challenge number 2 will be addressed in the next section. For the rest of this section, we will explain how step size is chosen for smooth, Lipschitz continuous function with Lipschitz constant $\beta$. Smooth function is not neccessarily convex thus we should not assume the property we are discussing here for any $\beta$-smooth function is only applicable to convex function.</p> <p>We end this section with a caveat: learning a global optimum is (usually) impractically hard. Under certain conditions, we can ensure that the gradient descent iterates converge to a local optimum. Also, under more relaxed conditions, we can ensure local convergence, i.e., that the iterates converge to a (global or local) optimum if the sequence is initialized close enough to the optimum.</p> <p>Recall that a differentiable function $f(x)$ has L-Lipschitz continuous gradients if</p> \[\begin{align} \|\nabla f(y) - \nabla f(x)\| \leq \beta \|y - x\| \quad \forall x, y \end{align}\] <p><strong>Lemma.</strong> Suppose that $f$ is differentiable and $\nabla f$ is $\beta$-Lipschitz. Then for every $x, x’ \in \mathbb{R}^n$,</p> \[\begin{align} f(x') \leq \hat{f}(x', x) = f(x) + \langle \nabla f(x), x' - x \rangle + \frac{\beta}{2} \|x' - x\|_2^2 \\ \quad\quad\;\;\; = \frac{\beta}{2} \left\| x' - \left(x - \frac{1}{\beta} \nabla f(x)\right) \right\|_2^2 + h(x). \end{align}\] <p>for some $h(x)$ that does not depend on $x’$ while $x$ is the base point. This baiscally means that we can always fit a quadratic upper bound on every point $x’$ of a Lipschitz continuous function that is $\beta$-smooth.</p> <p>We denote this upper bound as $\hat{f}(x’, x)$. The minimizer of this upper bound is</p> \[\begin{align} \arg \min_{x'} \hat{f}(x', x) = x - \frac{1}{\beta} \nabla f(x) \end{align}\] <p>This is exactly in the form of a gradient step in gradient descent. And because And because</p> \[\begin{align} f(x) = \hat{f}(x, x) \end{align}\] <p>And</p> \[\begin{align} \hat{f}(x'_{*}, x) \leq \hat{f}(x, x) \end{align}\] <p>Thus,</p> \[\begin{align} f(x'_*) \leq \hat{f}(x'_*, x) \leq \hat{f}(x, x) = f(x) \end{align}\] <p>Which imply that if we apply the gradient descent method with step size 1/$L$, we are guaranteed to produce the monotone sequence of function values $f(x_{k})$.</p> <h1 id="subgradient-method">Subgradient Method</h1> <p>To cope with the challenge number 2, we might consider subgradient method. Let’s consider the example problem:</p> \[\begin{align} f(x) = |x| \end{align}\] <p>At $x = 0$, unlike G.D., the gradient is not uniquely defined. Thus, we replace the concept gradient with another fancy term called subdifferential, which is actually a generalization of gradient. The definition of subdifferential is as follow:</p> <p>For a convex function $ f: \mathbb{R}^n \to \mathbb{R} $ the subdifferential $ \partial f(x) $ at a point $ x $ is the set of all vectors $ g \in \mathbb{R}^n $ that satisfy the subgradient inequality:</p> \[\begin{align} f(y) \geq f(x) + g^\top (y - x) \quad \forall y \in \mathbb{R}^n. \end{align}\] <p>Note that the subgradient inequality does not generally hold for non-convex problem. The key idea of subgradient method is then, for each update we do the same as G.D. except if we touch the non-differentiable point, we randomly pick a subdifferential in the set of valid subdifferentials. As the value oscillate around the non-differentiable point, we might apply some mechanism to shrink the step size. We repeat the iteration until it converges.</p> <p>The subgradient method algorithm can be summarized as follow：</p> <p>The subgradient method iteratively updates the iterate $z_k$ toward the minimum:</p> \[\begin{align} x_{k+1} = x_k - h_k g_k \end{align}\] <p>where:</p> <ul> <li> <p>$g_k$ is a subgradient at $z_k$.</p> </li> <li> <p>$h_k &gt; 0$ is the step size at iteration $k$.</p> </li> </ul> <p>Step Size Choices:</p> <ul> <li> <p>Fixed step size ($h_k = h$, $h = \frac{b}{2}$) may cause oscillation near the optimum; useful for illustration.</p> </li> <li> <p>Diminishing step size (e.g., $h_k = \frac{a}{k}$) for constants $a &gt; 0$, $b \geq 0$: Ensures convergence for bounded subgradients.</p> </li> </ul> <h2 id="optimality-condition-of-subgradient-methods">Optimality condition of subgradient methods</h2> <p>\(\text{Theorem.} \quad \text{Suppose that } f: \mathbb{R}^d \to \mathbb{R} \cup \{\infty\} \text{ is a convex function. Then, } x^* \text{ minimizes } f(x) \text{ if and only if } 0 \in \partial f(x^*)\)</p> <h2 id="convergence-rate-of-subgradient-method">Convergence rate of subgradient method</h2> <p>The main disadvantage to this approach is its relatively poor convergence rate.</p> <p>Let $x_<em>$ be the (global) minimizer of $f(x)$. In general, the convergence rate of the subgradient method for nonsmooth objective functions, in terms of the function value $f(x_k) - f(x_</em>)$, is:</p> \[O(1/\sqrt{k}).\] <p>The constants in the big-$O$ notation depend on various properties of the problem. The important point is that for even a moderate target accuracy \(\begin{align} f(x_k) - f(x_*) \leq \epsilon, \end{align}\) we will have to set $k = O(\epsilon^{-2})$ very large.</p> <h1 id="proximal-gradient-descent">Proximal Gradient Descent</h1> <p>Most objective functions such as that of ISTA problem is a composite of smooth and non-smooth functions.</p> \[\begin{align} F(x) = f(x)+g(x) \end{align}\] <p>where $f(x)$ is a $\beta$-smooth function and $g(x)$ is non-differentiable function such as $l_1$ norm. Therefore, the composite function $F(x)$ is not differentiable thus G.D. does not apply. The first recourse in this situation is to replace the gradient with a subgradient. The main disadvantage to this approach is its relatively poor convergence rate. Recall that thanks to self-tuning property of gradient descent method for smooth function, the convergence rate of G.D. is much better (at a rate of $O(1/k)$) than subgradient method. Can we draw inspiration from the gradient method to produce a more efficient algorithm for minimizing the composite function such that we can avoid slow convergence due to $x_{k+1} = x_k - h_k g_k$?</p> <p>As before, for the composite objective $F(x) = f(x) + g(x)$, we can construct an upper quadratic bound of $F(x)$ at $x_k$ using the smoothness of $f(x)$:</p> \[\hat{F}(x, x_k) = \frac{\beta}{2} \left\| x - \left( x_k - \frac{1}{\beta} \nabla f(x_k) \right) \right\|_2^2 + h(x_k) + g(x),\] \[\quad\quad\;\;\; = \hat{f}(x, x_k) + g(x).\] <p>where $\beta$ is the Lipschitz constant of the gradient of $f(x)$. Notice that $h(x_k)$ collects terms independent of $x$ (absorbing everything not affecting the minimization). This upper bound motivates the <strong>proximal gradient step</strong>:</p> <p>\(\begin{align} x_{k+1} &amp;= \arg\min_x\, \frac{\beta}{2} \left\| x - \left( x_k - \frac{1}{\beta} \nabla f(x_k) \right) \right\|_2^2 + g(x) \\ &amp;= \arg\min_x\, \frac{\beta}{2} \left\| x - w_k \right\|_2^2 + g(x) , \end{align}\) where $w_k = x_k - \frac{1}{\beta} \nabla f(x_k)$ which only depends on $x_k$.</p> <p>The minimization above is exactly the <strong>proximal operator</strong> of $g$ applied at the “gradient descent step” for $f$:</p> \[x_{k+1} = \mathrm{prox}_{g, \frac{1}{\beta}}\left( x_k - \frac{1}{\beta} \nabla f(x_k) \right) = \mathrm{prox}_{g, \frac{1}{\beta}}\left(w_k \right) = \arg\min_x\, \frac{\beta}{2} \left\| x - w_k \right\|_2^2 + g(x)\] <p>This combines the fast convergence of gradient descent for the smooth part with the regularization/control of the non-smooth part via the proximal operator. The resulting algorithm enjoys the same O(1/k) convergence rate as in the smooth case. Recognizing special structure in our problem of interest yields a significantly more accurate and eﬃcient algorithm. In fact, we can further apply acceleration method similar to acceleration method for purely smooth function to speed up the convergence! This is also the key reason how people may come up with Adam, one of the most popular optimizer in deep learning. We may talk about this in the future blog.</p> <h2 id="solving-lasso-by-proximal-gradient">Solving LASSO by proximal gradient</h2> <p>Let’s see how proximal operator looks like for LASSO problem:</p> \[\mathrm{prox}_{g, \frac{1}{\beta}}\left(w_k \right) = \arg\min_x\, \frac{\beta}{2} \left\| x - w_k \right\|_2^2 + g(x)\] <p>We can obtain the argument of the minimum by setting:</p> \[0 \in (x - w) + \lambda \partial \|x\|_1\] <p>where $w = w_k$, $\lambda = 1/\beta$, and $\partial |x|_1$ denotes the subdifferential of the $\ell_1$ norm. For each coordinate $i = 1, \ldots, n$:</p> <p>\(0 \in (x_i - w_i) + \lambda \partial |x_i| = \begin{cases} x_i - w_i + \lambda, &amp; x_i &gt; 0 \\ -w_i + \lambda[-1, 1], &amp; x_i = 0 \\ x_i - w_i - \lambda, &amp; x_i &lt; 0 \end{cases}\) Therefore, the solution to this optimality condition is the <strong>soft-thresholding function applied element-wise</strong>: \(x_i^* = \mathrm{soft}(w_i, \lambda) \triangleq \mathrm{sign}(w_i) \max(|w_i| - \lambda, 0) \qquad i = 1, \ldots, n.\)</p> <p>The pseudocode of PGD for LASSO problem is as follow:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pgd_for_lasso-480.webp 480w,/assets/img/pgd_for_lasso-800.webp 800w,/assets/img/pgd_for_lasso-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/pgd_for_lasso.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>As the iteration goes on, more and more value in the solution vector may go zero to promote sparsity. The soft-threshold is determined by $\frac{\lambda}{\beta}$ (replace $L$ by $\beta$ in the picture)</p> <h2 id="similarity-of-proximal-operator-and-nonlinear-function-in-deep-network">Similarity of proximal operator and nonlinear function in deep network</h2> <p>The soft-thresholding in LISTA looks notably similar to ReLU in deep network.</p> <h1 id="how-solution-to-lasso-inspire-more-efficient-solution-to-basis-pursuit">How solution to LASSO inspire more efficient solution to basis pursuit</h1> <p>When I first learn about these 2 problems, most textbook would introduce them as following roughly: LASSO is a variant of the BP, considering the noisy case in which the original signal is contaminated by moderate Gaussian noise $\mathcal{z}$.</p> <p>Basis Pursuit:</p> \[\begin{aligned} &amp;\min_{x} \quad &amp;&amp; \|x\|_1 \\ &amp;\text{subject to} \quad &amp;&amp; Ax = y. \end{aligned}\] <p>LASSO:</p> \[\begin{align} \min_{x} \ \frac{1}{2} \|y - Ax\|_2^2 + \lambda \|x\|_1 \end{align}\] <p>where the observations $y$ are generated as $y = Ax_0 + z$, for some unknown sparse $x_0$ and noise $z$.</p> <p>However, after I went through the optimization textbook or chapters in related textbook over and over again, I have found that there is a deeper and valuable insight. First of all, let’s recall the core difference between 2 problems. BP or BPDN enforce equality or inequality constraints while LASSO does not strictly do so. In LASSO, it is up to us to control the $\lambda$.</p> <p>Because of the requirement to enforce equality constraint on BP, the first intuitive method comes to people mind is to apply projected subgradient method to solve BP. The key idea of BP is to alternate between subgradient steps and projection operation. The convergence rate thus is mainly determined by subgradient step for non-smooth and non-strongly convex $|x|_1$, given by:</p> \[O(1/\sqrt{k}).\] <p>The constants in the big-$O$ notation depend on various properties of the problem. The important point is that for even a moderate target accuracy \(\begin{align} f(x_k) - f(x_*) \leq \epsilon, \end{align}\) we will have to set $k = O(\epsilon^{-2})$ very large.</p> <p>Apart from using projected subgradient method to solve this problem, another approach is to solve a series of problems of this type :</p> \[\begin{align} \min_x \ \|x\|_1 + \frac{\mu}{2} \|y - Ax\|_2^2 \end{align}\] <p>for an increasing sequence of $\mu_i \to \infty$ to enforce the equality.</p> <p>Now, the deeper insight I have mentioned will be explained starting from here.</p> <p>First of all, it is obvious that for each $\mu_i$, the problem is exactly a LASSO problem, which can be solved by proximal gradient descent (PGD).</p> <p>PGD, unlike projected subgradient descent, benefit from the smooth part $|y - Ax|_2^2$ and thus the convergence rate is $O(1/k)$. It also allows us to utilize acceleration techniques that were designed for smooth functions and lead to much more scalable and fast-converging algorithms, with convergence rates much better than the generic situation. Therefore, in each iteration for each $\mu_i$. we solve the problem more efficiently. However, as the weight $\mu_i$ increases, the corresponding BPDN problem becomes increasingly ill-conditioned and hence algorithms converge slower. This is because for first-order methods such as PG and APG, the rate of convergence is dictated by how quickly the gradient $\nabla(\mu f) = \mu A^T (A x - y)$ can change from point-to-point, which is measured through the Lipschitz constant.</p> \[\begin{align} L_{\nabla_{\mu}f} = \mu \|\mathbf{A}\|_2^2. \end{align}\] <p>This increases linearly with $\mu$: The larger $\mu$ is, the harder the unconstrained problem is to solve! We can then employ the augmented Lagrange multiplier (ALM) technique to alleviate this diﬃculty.</p> <h1 id="connection-between-ista-and-deep-network">Connection between ISTA and deep network</h1> <p>Recall that in each layer of forward pass of feature transformation in ISTA is given by the following process:</p> \[\begin{align} \mathbf{Z}_{\ell+1} = S_{\eta, \lambda} \left( \mathbf{Z}_\ell - 2\eta (\mathbf{A}_\ell)^\top \left( \mathbf{A}_\ell \mathbf{Z}_\ell - \mathbf{X} \right) \right),\quad \forall \ell \in [L]. \end{align}\] <p>It looks like the forward pass of a deep neural network with weights given by $\mathbf{A}$. The first part of this blog</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/what-to-learn/">What to learn and introduction to problems solvable by analytical approach</a> </li> <br> <br> <div id="giscus_thread"> <script defer src="/assets/js/giscus-setup.js"></script> <noscript> Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Ka Lok Ng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/js/diff2html-ui.min.js" integrity="sha256-eU2TVHX633T1o/bTQp6iIJByYJEtZThhF9bKz/DcbbY=" crossorigin="anonymous"></script> <script defer src="/assets/js/diff2html-setup.js?80a6e52ce727518bbd3aed2bb6ba5601" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.js" integrity="sha256-MgH13bFTTNqsnuEoqNPBLDaqxjGH+lCpqrukmXc8Ppg=" crossorigin="anonymous"></script> <script defer src="/assets/js/leaflet-setup.js?b6313931e203b924523e2d8b75fe8874" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script> <script defer src="/assets/js/chartjs-setup.js?183c5859923724fb1cb3c67593848e71" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/theme/dark-fresh-cut.js" integrity="sha256-sm6Ui9w41++ZCWmIWDLC18a6ki72FQpWDiYTDxEPXwU=" crossorigin="anonymous"></script> <script defer src="/assets/js/echarts-setup.js?738178999630746a8d0cfc261fc47c2c" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script defer src="/assets/js/vega-setup.js?7c7bee055efe9312afc861b128fe5f36" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script src="/assets/js/typograms.js?062e75bede72543443762dc3fe36c7a5"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>