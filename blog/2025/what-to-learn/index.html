<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> What to learn and introduction to problems solvable by analytical approach | Ka Lok Ng </title> <meta name="author" content="Ka Lok Ng"> <meta name="description" content="Idealistic models that inspire deep network structures"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ngkel.github.io/blog/2025/what-to-learn/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.css" integrity="sha256-q9ba7o845pMPFU+zcAll8rv+gC+fSovKsOoNQ6cynuQ=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css" integrity="sha256-Oppd74ucMR5a5Dq96FxjEzGF7tTw2fZ/6ksAqDCM8GY=" crossorigin="anonymous" media="screen and (prefers-color-scheme: light)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" integrity="sha256-nyCNAiECsdDHrr/s2OQsp5l9XeY2ZJ0rMepjCT2AkBk=" crossorigin="anonymous" media="screen and (prefers-color-scheme: dark)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/css/diff2html.min.css" integrity="sha256-IMBK4VNZp0ivwefSn51bswdsrhk0HoMTLc2GqFHFBXg=" crossorigin="anonymous"> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "What to learn and introduction to problems solvable by analytical approach",
            "description": "Idealistic models that inspire deep network structures",
            "published": "August 18, 2025",
            "authors": [
              
              {
                "author": "Ng Ka Lok",
                "authorURL": "ngkel.github.io",
                "affiliations": [
                  {
                    "name": "Ex-HKU, Foodpanda",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ka Lok</span> Ng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">miscellaneous </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>What to learn and introduction to problems solvable by analytical approach</h1> <p>Idealistic models that inspire deep network structures</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#what-learning-is-all-about-or-at-least-what-are-modern-methods-actually-doing">What learning is all about? Or at least, what are modern methods actually doing?</a> </div> <div> <a href="#learning-problems-that-are-solvable-by-analytical-approach">Learning problems that are solvable by analytical approach</a> </div> <ul> <li> <a href="#pca-finding-the-single-subspace-that-best-fits-the-data">PCA - finding the single subspace that best fits the data</a> </li> <li> <a href="#power-iteration-workhorse-of-pca">Power Iteration - Workhorse of PCA</a> </li> <li> <a href="#limitation-of-pca">Limitation of PCA</a> </li> <li> <a href="#mixtures-of-subspaces-and-sparsely-used-dictionaries">Mixtures of Subspaces and Sparsely-Used Dictionaries</a> </li> <li> <a href="#overcomplete-dictionary-learning">Overcomplete dictionary learning</a> </li> </ul> <div> <a href="#learned-ista">Learned ISTA</a> </div> </nav> </d-contents> <h1 id="what-learning-is-all-about-or-at-least-what-are-modern-methods-actually-doing">What learning is all about? Or at least, what are modern methods actually doing?</h1> <p>In nature, data in high dimensional space has low dimensional support. Data lie precisely on geomoetric structures such as subspaces or surfaces. Some belong to a subspace, some belong to a mixture of subspaces or manifolds, most of the time we don’t know the dimension of each component. Such constraint make data highly dependent on one another. In other words, natural data is predictable. Completion is perhaps another name of prediction. Denoise and error correction, are tasks that are enabled by the fact that data has constraints.</p> <p>Thus, we can say that learning is useful as it aims at, first, identifying and representing the low-dimensional structure of data. Next, we utilize the low-dimensional structure to inference for useful downstream tasks.</p> <h1 id="why-learning-the-model-px-is-useful---bayesian-inference-point-of-view">Why learning the model $p(x)$ is useful? - Bayesian inference point of view</h1> <p>Most modern machine learning methods attempt to identify the low dimensional structure of the data distribution, either by getting hold of thosse distributions analytically (e.g. PCA) or empirically(e.g. Neural Network). Why is that useful? Such learnt low-dimensional structure of data is in fact the model of the world, and in Bayesian language, the prior. Almost all practical applications can be viewed as a special case of the following inference problem: given an observation $y$ that depends on $x$,</p> \[\begin{align} y = h(x) + w \end{align}\] <p>where $h(\cdot)$ represents the measurements of a part of $x$ or its certain observed attributes and $w$ represent the measurement noise, solve the “inverse problem” of obtaining the most likely estimate $\hat{x}(y)$ of $x$. Solving this is equivalent to</p> \[\begin{align} \hat{x} = \arg\max_{x} p(x\mid y) \end{align}\] <p>And by Bayes’ rule:</p> \[\begin{align} p(x \mid y) = \frac{p(y \mid x) p(x)}{p(y)} \end{align}\] <p>Therefore we can rewrite the problem as: \(\begin{align} \hat{x} = \arg\max_{x} [\log p(y \mid x) + \log p(x)] \end{align}\)</p> <p>Last but not the least, another application of get ahold of the conditional probability $p(x \mid y)$ is computing the conditional expectation estimate:</p> \[\begin{align} \hat{x} = \mathbb{E}[x \mid y] = \int x\, p(x \mid y)\, dx \end{align}\] <p>(Please confirm if (5) is a different application than (4)).</p> <p>Our questions then become:</p> <ol> <li>How to learn and represent the data</li> <li>How to make use of the representation obtained from 1 to conduct useful inference.</li> </ol> <p><strong><em>In this series of blogs, we tackle these questions by claiming that we learn and represent the data as distribution $p(x)$ with low dimenstional structure. Then we make use of $p(x)$ for inferencing.</em></strong></p> <h1 id="learning-problems-that-are-solvable-by-analytical-approach">Learning problems that are solvable by analytical approach</h1> <p>Analytical approaches involve explicit low-dimensional structure being assumed while empirical do not. In this section, we discuss classical problem settings and solutions for learning models with assumption that data having geometrically (nearly, piece-wise) linear structures and statistically independent components. Such model is what we call analytical models. Such assumptions offer us efficient algorithm with provable efficiency guarantees for processing data at scale. You may ask why we are learning these models when the real world data is far more complex? We will see in the upcoming blogs that many modern deep learning architectures for complex data actually have structures that are similar to algorithm for modeling data with linear and independent structures, such as overcomplete dictionary learning.</p> <p>Learning distribution with idealistic, linear models with independent structures has efficient solutions. Examples include:</p> <ol> <li>PCA</li> <li>ICA</li> <li>Sparse Coding and Overcomplete Dictionary Learning</li> </ol> <p>Distributions for the above modeling problems can be learned and represented explicitly. We will see later that general distribution does not yield efficient analytical solution and cannot be represented explicitly but they can be learnt implicitly as a denoiser.</p> <p><strong><em>Reminder on what we mean by explicit family of parametric model: The model class that is directly and mathematically specified, for example, “all Gaussians with arbitary mean and covariance”. When learning this type of model, we fix the form of the model and tune the set of parameters. Neural networks on the other hands, are considered empirical approaches without clearly stating analytically the form of distribution they are trying to learn. One might argue that the network architecture may limit or define the type of distribution they are able to learn, but the design process of neural networks does not involve explicitly define the form of distribution with clear mathematical descriptions.</em></strong></p> <h2 id="pca---finding-the-single-subspace-that-best-fits-the-data">PCA - finding the single subspace that best fits the data</h2> <p>In PCA, data is assumed, explicitly, to live on a single Gaussian subspace $\mathcal{S} \subseteq \mathbb{R}^D$ of dimension $d$ with basis represented by an orthonormal matrix $\mathbf{U} \in O(D, d) \subseteq \mathbb{R}^{D \times d}$ such that the columns of $\mathbf{U}$ span $\mathcal{S}$. The problem of learning thus becomes:</p> <p>Given observed data ${x_i}_{i=1}^N$, finding the orthonoraml matrix $\mathbf{U} \in O(D,d)$ such that</p> \[\begin{align} \mathbf{x}_i = \mathbf{U} \mathbf{z}_i + \boldsymbol{\varepsilon}_i, \quad \forall i \in [N] \end{align}\] <p>where</p> \[\begin{align} \{\mathbf{z}_i\}_{i=1}^N \subseteq \mathbb{R}^d, \quad d \ll D \end{align}\] <p>The optimization problem for finding the optimal subspace $\mathbf{U}^*$ can be formulated as:</p> \[\begin{align} \arg \min_{\tilde{\mathbf{U}}} \frac{1}{N} \sum_{i=1}^{N} \| \mathbf{x}_i - \tilde{\mathbf{U}} \tilde{\mathbf{U}}^T \mathbf{x}_i \|_2^2 &amp;= \arg \max_{\tilde{\mathbf{U}}} \frac{1}{N} \sum_{i=1}^{N} \| \tilde{\mathbf{U}}^T \mathbf{x}_i \|_F^2 \\ &amp;= \arg \max_{\tilde{\mathbf{U}}} \text{tr} \left\{ \tilde{\mathbf{U}}^T \left( \frac{\mathbf{X} \mathbf{X}^T}{N} \right) \tilde{\mathbf{U}} \right\} \end{align}\] <p>That is, the optimal solution $(\mathbf{U}^<em>, {\mathbf{z}^</em><em>i}</em>{i=1}^N)$ to the above optimization problem has $\mathbf{z}^<em>_i = (\mathbf{U}^</em>)^T \mathbf{x}_i$. That’s why the optimization problem above becomes a problem over $\mathbf{U}$ only. The solution of the above optimization problem is given by the top $d$ eigenvectors of:</p> \[\begin{align} \frac{\mathbf{X} \mathbf{X}^T}{N} \end{align}\] <p>It is worth noting that projection matrix:</p> \[\begin{align} \mathbf{U}^* (\mathbf{U}^*)^T \approx \mathbf{U} \mathbf{U}^T \end{align}\] <p>can project the noisy data point $\mathbf{x}_i$ onto subspace $\mathcal{S}$.</p> <p>Reminder of linear algebra concept: Matrix multiplication, let’s say multiplied by $\mathbf{U}$ has 3 interpretations:</p> <ol> <li>Transformation of a vector</li> <li>Express original vector $\mathbf{x}$ in terms of a vector based on coordinate provided by $\mathbf{U}$.</li> </ol> <h2 id="power-iteration---workhorse-of-pca">Power Iteration - Workhorse of PCA</h2> <p>Computing full SVD for a matrix is computationally intensive. There is, however, an efficient way to comput the eigenvector of a symmetric, positive semidefinite matrix $\mathbf{M}$. The method is called power iteration.</p> <p>Power iteration is a fundamental algorithm for finding the dominant eigenvector of a matrix $\mathbf{M}$. The key insight is that repeatedly applying the matrix and normalizing will converge to the eigenvector corresponding to the largest eigenvalue.</p> <p><strong>The Algorithm.</strong></p> <p>Assume that $\lambda_1 &gt; \lambda_i$ for all $i &gt; 1$. We want to find the fixed point of:</p> \[\begin{align} \mathbf{w} = \frac{\mathbf{M}\mathbf{w}}{\|\mathbf{M}\mathbf{w}\|_2} \end{align}\] <p>We can solve this using the following iterative procedure:</p> \[\begin{align} \mathbf{v}_0 \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), \quad \mathbf{v}_{t+1} = \frac{\mathbf{M}\mathbf{v}_t}{\|\mathbf{M}\mathbf{v}_t\|_2} \end{align}\] <p><strong>Why does this work?</strong></p> <p><strong>Proof Sketch.</strong></p> <p>For any iteration $t$, we have:</p> \[\begin{align} \mathbf{v}_t = \frac{\mathbf{M}^t \mathbf{v}_0}{\|\mathbf{M}^t \mathbf{v}_0\|_2} \end{align}\] <p>Let’s decompose the initial vector $\mathbf{v}_0$ in the eigenbasis of $\mathbf{M}$:</p> \[\begin{align} \mathbf{v}_0 = \sum_{i=1}^D \alpha_i \mathbf{w}_i \end{align}\] <p>where $\mathbf{M}\mathbf{w}_i = \lambda_i \mathbf{w}_i$ and $\lambda_1 &gt; \lambda_2 \geq \dotsb \geq \lambda_D \geq 0$.</p> <p>Substituting this into our iteration formula:</p> \[\begin{align} \mathbf{v}_t = \frac{ \sum_{i=1}^D \lambda_i^t \alpha_i \mathbf{w}_i }{ \left\| \sum_{i=1}^D \lambda_i^t \alpha_i \mathbf{w}_i \right\|_2 } \end{align}\] <p>As $t \to \infty$, since $\lambda_1 &gt; \lambda_i$ for $i &gt; 1$, the terms with $i &gt; 1$ vanish exponentially faster than the first term. This gives us:</p> \[\begin{align} \lim_{t \to \infty} \mathbf{v}_t = \frac{ \alpha_1 \mathbf{w}_1 }{ |\alpha_1| } = \operatorname{sign}(\alpha_1)\mathbf{w}_1 \end{align}\] <p>Therefore, $\mathbf{v}_t$ converges to a unit eigenvector of $\mathbf{M}$ corresponding to the largest eigenvalue.</p> <p><strong>Estimating the eigenvalue.</strong></p> <p>Once we have the eigenvector, the corresponding eigenvalue can be estimated as:</p> \[\begin{align} \lambda_1 \approx \mathbf{v}_t^\top \mathbf{M} \mathbf{v}_t \end{align}\] <p>This quantity converges to $\lambda_1$ at the same rate as the eigenvector convergence.</p> <p>Implementation:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
</pre></td> <td class="code"><pre><span class="k">import</span> <span class="n">numpy</span> <span class="n">as</span> <span class="n">np</span>

<span class="n">def</span> <span class="n">power_iteration</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span><span class="o">:</span>
    <span class="s">"""</span><span class="err">
</span><span class="s">    Compute the dominant eigenvalue and eigenvector of a symmetric positive semidefinite matrix</span><span class="err">
</span><span class="s">    using the Power Iteration method.</span><span class="err">

</span><span class="s">    Parameters:</span><span class="err">
</span><span class="s">    M (numpy.ndarray): Symmetric positive semidefinite matrix</span><span class="err">
</span><span class="s">    num_iterations (int): Maximum number of iterations</span><span class="err">
</span><span class="s">    tol (float): Convergence tolerance</span><span class="err">

</span><span class="s">    Returns:</span><span class="err">
</span><span class="s">    tuple: (eigenvalue, eigenvector)</span><span class="err">
</span><span class="s">    """</span>
    <span class="cp"># Input validation
</span>    <span class="k">if</span> <span class="n">not</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">M</span><span class="p">.</span><span class="n">T</span><span class="p">)</span><span class="o">:</span>
        <span class="n">raise</span> <span class="n">ValueError</span><span class="p">(</span><span class="s">"Matrix must be symmetric"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">not</span> <span class="n">np</span><span class="p">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">M</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="n">tol</span><span class="p">)</span><span class="o">:</span>  <span class="err">#</span> <span class="n">Allow</span> <span class="n">small</span> <span class="n">negative</span> <span class="n">values</span> <span class="n">due</span> <span class="n">to</span> <span class="n">numerical</span> <span class="n">errors</span>
        <span class="n">raise</span> <span class="n">ValueError</span><span class="p">(</span><span class="s">"Matrix must be positive semidefinite"</span><span class="p">)</span>

    <span class="cp"># Initialize random vector
</span>    <span class="n">n</span> <span class="o">=</span> <span class="n">M</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>  <span class="err">#</span> <span class="n">Normalize</span> <span class="n">initial</span> <span class="n">vector</span>

    <span class="cp"># Power iteration
</span>    <span class="k">for</span> <span class="n">_</span> <span class="n">in</span> <span class="n">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">)</span><span class="o">:</span>
        <span class="cp"># Matrix-vector multiplication
</span>        <span class="n">v_new</span> <span class="o">=</span> <span class="n">M</span> <span class="err">@</span> <span class="n">v</span>

        <span class="cp"># Compute eigenvalue (Rayleigh quotient)
</span>        <span class="n">eigenvalue</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">v_new</span><span class="p">)</span>

        <span class="cp"># Normalize the new vector
</span>        <span class="n">v_new_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v_new</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">v_new_norm</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="o">:</span>  <span class="err">#</span> <span class="n">Check</span> <span class="k">for</span> <span class="n">zero</span> <span class="n">vector</span>
            <span class="n">raise</span> <span class="n">ValueError</span><span class="p">(</span><span class="s">"Power iteration converged to zero vector"</span><span class="p">)</span>
        <span class="n">v_new</span> <span class="o">=</span> <span class="n">v_new</span> <span class="o">/</span> <span class="n">v_new_norm</span>

        <span class="cp"># Check convergence
</span>        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">v_new</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tol</span> <span class="n">or</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v</span> <span class="o">+</span> <span class="n">v_new</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="o">:</span>
            <span class="k">break</span>

        <span class="n">v</span> <span class="o">=</span> <span class="n">v_new</span>

    <span class="k">return</span> <span class="n">eigenvalue</span><span class="p">,</span> <span class="n">v</span>

<span class="cp"># Example usage
</span>
<span class="cp"># Create a sample 3x3 symmetric positive semidefinite matrix
</span><span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>

<span class="cp"># Run power iteration
</span><span class="n">eigenvalue</span><span class="p">,</span> <span class="n">eigenvector</span> <span class="o">=</span> <span class="n">power_iteration</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>

<span class="n">print</span><span class="p">(</span><span class="s">"Dominant eigenvalue:"</span><span class="p">,</span> <span class="n">eigenvalue</span><span class="p">)</span>
<span class="n">print</span><span class="p">(</span><span class="s">"Corresponding eigenvector:"</span><span class="p">,</span> <span class="n">eigenvector</span><span class="p">)</span>

<span class="cp"># Verify result
</span><span class="n">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Verification:"</span><span class="p">)</span>
<span class="n">print</span><span class="p">(</span><span class="s">"M @ eigenvector:"</span><span class="p">,</span> <span class="n">M</span> <span class="err">@</span> <span class="n">eigenvector</span><span class="p">)</span>
<span class="n">print</span><span class="p">(</span><span class="s">"eigenvalue * eigenvector:"</span><span class="p">,</span> <span class="n">eigenvalue</span> <span class="o">*</span> <span class="n">eigenvector</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <h2 id="limitation-of-pca">Limitation of PCA</h2> <p>PCA can denoise data by projecting original data on a single principal subspace. However, it has 2 key limitations:</p> <ol> <li>It fails to model nonlinear structure</li> <li>Real world data comes from multiple subspaces or surfaces. PCA also fails to model.</li> </ol> <p>The following 3D interactive plots demonstrate visually and computationally why PCA is limited when the data lies on a nonlinear manifold (e.g., a sine wave in 3D). The PCA algorithm is applied to the noisy data, attempting to find the best linear subspace that fits the data. PCA projects the data onto the top 2 principal components, which are linear. The denoised result is obtained by reconstructing the data from this 2D linear subspace. However, the learned model does not offer generative model for nonlinear distribution, since only variance along the principal direction is captured. That is not enough to describe a sinusoidal relationship.</p> <div class="l-page" style="display: flex; justify-content: center; gap: 20px; flex-wrap: wrap;"> <div style="flex: 1; min-width: 400px;"> <iframe src="/assets/plotly/blog_2025_08_18/pca_sin_wave_noisy.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div style="flex: 1; min-width: 400px;"> <iframe src="/assets/plotly/blog_2025_08_18/pca_sin_wave_denoised.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> </div> <div> <iframe src="/assets/plotly/blog_2025_08_18/pca_sin_wave_generative_model.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <h2 id="mixtures-of-subspaces-and-sparsely-used-dictionaries">Mixtures of Subspaces and Sparsely-Used Dictionaries</h2> <p>A key limitation is the assumption of a single linear subspace that is responsible for generating the structured observations. In many practical applications, structure generated by a mixture of distinct low-dimensional subspaces provides a more realistic model. For example, consider a video sequence that captures the motion of several distinct objects, each subject to its own independent displacement.</p> <p>In mixture of Gaussians, the random variable is generated by, randomly select a Gaussian from K Gaussians, then randomly select a random variable in that selected Gaussian.</p> <p>The probability density function can be written as: \(\begin{align} p(\mathbf{x}_n) = \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \end{align}\)</p> <p>Please don’t confuse this with superposition: \(\begin{align} \mathbf{x} = \sum_{i=1}^{n} w_i \mathbf{x}_i, \quad \mathbf{x}_i \sim \mathcal{N}(\mathbf{0}, \mathbf{U}_i \mathbf{U}_i^\top) \end{align}\)</p> <p>After assuming a mixture of Gaussian model, our task is to learn $\mathbf{U}$ which satisfy the following:</p> \[\begin{align} \mathbf{x} = \begin{bmatrix} | &amp; &amp; | \\ \mathbf{U}_1 &amp; \cdots &amp; \mathbf{U}_K \\ | &amp; &amp; | \end{bmatrix} \begin{bmatrix} \mathbf{z}_1 \\ \vdots \\ \mathbf{z}_K \end{bmatrix} = \mathbf{U}\, \mathbf{z}, \quad \left\| \begin{bmatrix} \|\mathbf{z}_1\|_2 \\ \vdots \\ \|\mathbf{z}_K\|_2 \end{bmatrix} \right\|_0 = 1 \end{align}\] <ul> <li>\(\mathbf{U} \in \mathbb{R}^{D \times (K d)}\) is a <em>dictionary</em> consisting of the collection of codewords \(\{\mathbf{U}_i\}_{i=1}^K\).</li> <li>\(\mathbf{z} \in \mathbb{R}^{K d}\) is a <em>lifted vector</em> that is \(d\)-sparse, with support on one block of size \(d\).</li> </ul> <p>where $\mathbf{U} \in \mathbb{R}^{D \times Kd}$</p> <p>A relaxation is to assume matrix $\mathbf{U} \in \mathbb{R}^{D \times m}$ where $m$ may be smaller or larger than $D$. This leads to sparse dictionary learning problem. There are both geometric and physical/modeling motivations for considering $d \ll m$. The problem is thus turned to be an overcomplete dictionary learning problem. The motivation includes having a richer mixtures of subspaces and some computational experiments in the past reveals the additional modeling power conferred by an overcomplete representation, for example, Bruno Olshausen’s paper on overcomplete dictionary learning.</p> <h2 id="dictionary-learning-vs-traditional-by-design-method">Dictionary Learning vs Traditional “by-design” method</h2> <h3 id="advantages-of-dictionary-learning-over-traditional-by-design-methods">Advantages of Dictionary Learning Over Traditional “By-Design” Methods</h3> <p>Traditional “by-design” methods for constructing bases—such as the Fourier Transform, Wavelet Transform, or even principal component analysis (PCA)—use mathematically engineered bases that are optimal for broad classes of signals, but agnostic to the specific data at hand. Dictionary learning, in contrast, seeks to <strong>adapt the basis (the dictionary) directly from the dataset itself</strong>, enabling several key advantages:</p> <h4 id="1-data-adaptivity">1. <strong>Data Adaptivity</strong> </h4> <ul> <li> <strong>By-Design Methods:</strong> Bases like Fourier are fixed a priori; they are not tailored for the specifics of your data.</li> <li> <strong>Dictionary Learning:</strong> Learns a dictionary directly from data, giving basis elements that closely match typical data patterns (edges, textures, motifs, etc.). This often leads to sparser and more informative representations.</li> </ul> <h4 id="2-sparsity-and-efficiency">2. <strong>Sparsity and Efficiency</strong> </h4> <ul> <li> <strong>By-Design Methods:</strong> Real-world signals are often not sparsely represented in global bases like Fourier or even wavelets. For example, Discrete Cosine Transform had been applied on image compression but in fact it was a poor method to deal with images, since most real-world images have sharp edges which result in the image being not band-limited as a signal, thus it is poorly represented by the transformed representation.</li> <li> <strong>Dictionary Learning:</strong> By adapting to data, learned dictionaries can represent each sample using only a few active atoms (columns), which can improve compression, denoising, and interpretability.</li> </ul> <h4 id="3-expressivity-and-overcompleteness">3. <strong>Expressivity and Overcompleteness</strong> </h4> <ul> <li> <strong>By-Design Methods:</strong> Predefined bases are usually complete or orthogonal but not overcomplete, limiting the richness of representation.</li> <li> <strong>Dictionary Learning:</strong> Allows learning <strong>overcomplete dictionaries</strong> (more atoms than dimensions), enabling richer, more flexible representations and capturing more complex or nuanced structures in the data.</li> </ul> <p><strong>In essence, dictionary learning empowers us to discover the most efficient building blocks for our data, often leading to better representations and improved performance in practical machine learning and signal processing applications.</strong></p> <h2 id="overcomplete-dictionary-learning">Overcomplete dictionary learning</h2> <p>To learn $\mathbf{U}$, the corresponding optimization problem can be written as follow:</p> \[\begin{align} \min_{\tilde{\mathbf{Z}},\, \tilde{\mathbf{A}}: \|\tilde{\mathbf{A}}_j\|_2 \leq 1} \Big\{\, \|\mathbf{X} - \tilde{\mathbf{A}}\, \tilde{\mathbf{Z}}\,\|_F^2 + \lambda \|\tilde{\mathbf{Z}}\|_1 \, \Big\} \end{align}\] <p>There are multiple things worth a separate blog to discuss in this optimization problem:</p> <ol> <li>The constraint of having norm in each basis vector of matrix $\mathbf{A}$</li> <li>Such minimization problem is non-convex. How to minimize it with alternating minization of $\mathbf{Z}$ and $\mathbf{A}$?</li> <li>How does the number of rows affect the “quality” of the features learnt?</li> </ol> <p>Despite the dictionary learning problem being a nonconvex problem, it is easy to see that fixing one of the 2 unknowns and optimize the other makes the problem convex and easy to solve. It has been shown that alternating minimization type algorithms indeed converge to the correct solution, at least locally. The algorithm is as follow:</p> \[\begin{align} \mathbf{Z}^{\ell+1} = S_{\eta\lambda}\left(\mathbf{Z}^{\ell} - 2 \eta\, \mathbf{A}^{+\top} \left(\mathbf{A}^+ \mathbf{Z}^{\ell} - \mathbf{X}\right)\right),\quad \mathbf{Z}^1=\mathbf{0},\quad \forall\, \ell \in [L] \end{align}\] \[\begin{align} \mathbf{Z}^+ = \mathbf{Z}^L \end{align}\] \[\begin{align} \mathbf{A}^{t+1} = \mathrm{proj}_{\|(\cdot)_j\|_2 \leq 1,\, \forall j}\left(\mathbf{A}^t - 2\nu\, (\mathbf{A}^t \mathbf{Z}^+ - \mathbf{X}) (\mathbf{Z}^+)^\top \right) \end{align}\] \[\begin{align} (\mathbf{A}^1)_j \sim \mathrm{i.i.d.}\, \mathcal{N}\left( \mathbf{0}, \frac{1}{D}\mathbf{I} \right),\quad \forall j \in [m] ,\quad \forall t \in [T] \end{align}\] \[\begin{align} \mathbf{A}^+ = \mathbf{A}^T \end{align}\] <p>The main insight from the above algorithm is that, when we fix $\mathbf{A}$, the ISTA update of $\mathbf{Z}^{\ell}$ looks like the forward pass of neural networks. Then the using the thrid line to update $\mathbf{A}$ based on the residual of using the current estimate of the sparse codes $\mathbf{Z}$</p> <p>(Draft) Equation 18 and 19 are ISTA algorithm for LASSO problem.</p> <h2 id="ista---an-algorithm-for-solving-sparse-coding-in-overcomplete-dictionary-learning">ISTA - An algorithm for solving sparse coding in overcomplete dictionary learning</h2> <h1 id="learned-ista-lista">Learned ISTA (LISTA)</h1> <p>The above deep-network interpretation of the alternating miniziation is more conceptual than practical, as the process can be rather inefficient and take many layers or iterations to converge. This is because we are trying to infer both $\mathbf{A}$ and $\mathbf{Z}$ from $\mathbf{X}$. The problem can be simplified in a supervised setting where $(\mathbf{X}, \mathbf{Z})$ are provided and use, say, back propagation type of algorithm to learn $\mathbf{A}$.</p> <p>The same methodology can be used as a basis to understand the representations computed in other network architectures, such as Transformer. Modern unsupervised learning paradigms are generally more data friendly but still, LISTA algorithm provide a useful practical basis for us to interpret the features in pretrained large-scale deep networks.</p> <p>The connection between low-dimensional-structure-seeking optimization algorithms and deep network architecture suggests scalable and natural neural learning architectures which may even be usable without backpropagation.</p> <h2 id="sparse-autoencoders">Sparse Autoencoders</h2> <p>Unlike LISTA, which requires large amounts of labeled $(\mathbf{X}, \mathbf{Z})$, unsupervised learning paradigm is more data friendly. We can use our development of the LISTA algorithm above to understand common practices in this field of research.</p> <p>Connection of LISTA to Sparse Autoencoder</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/optimization-basics/">Optimization from basics to ISTA</a> </li> <br> <br> <div id="giscus_thread"> <script defer src="/assets/js/giscus-setup.js"></script> <noscript> Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Ka Lok Ng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/js/diff2html-ui.min.js" integrity="sha256-eU2TVHX633T1o/bTQp6iIJByYJEtZThhF9bKz/DcbbY=" crossorigin="anonymous"></script> <script defer src="/assets/js/diff2html-setup.js?80a6e52ce727518bbd3aed2bb6ba5601" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.js" integrity="sha256-MgH13bFTTNqsnuEoqNPBLDaqxjGH+lCpqrukmXc8Ppg=" crossorigin="anonymous"></script> <script defer src="/assets/js/leaflet-setup.js?b6313931e203b924523e2d8b75fe8874" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script> <script defer src="/assets/js/chartjs-setup.js?183c5859923724fb1cb3c67593848e71" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/theme/dark-fresh-cut.js" integrity="sha256-sm6Ui9w41++ZCWmIWDLC18a6ki72FQpWDiYTDxEPXwU=" crossorigin="anonymous"></script> <script defer src="/assets/js/echarts-setup.js?738178999630746a8d0cfc261fc47c2c" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script defer src="/assets/js/vega-setup.js?7c7bee055efe9312afc861b128fe5f36" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script src="/assets/js/typograms.js?062e75bede72543443762dc3fe36c7a5"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>